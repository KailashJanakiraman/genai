{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# FRAMEWORK - langchain\n",
        "\n"
      ],
      "metadata": {
        "id": "ZSyHeJe1zoSM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Y-ovpnP3-FT"
      },
      "outputs": [],
      "source": [
        "# how to interact wiht LLMs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 . API key\n",
        "# 2 . model name - API\n",
        "# 3 . PROMPT\n",
        "# 4 . SDK OR framework.  -- LANGCHAIN"
      ],
      "metadata": {
        "id": "x2wMDoaq4OYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ['GOOGLE_API_KEY']=userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "UkAvAtpT7cqU"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Langchain"
      ],
      "metadata": {
        "id": "JSgODQ5d43jn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU \"langchain[google-genai]\""
      ],
      "metadata": {
        "id": "ANKsGmBP5U3F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a2dc9a9-4b0f-4976-c3ba-d4c28656698d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/473.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m368.6/473.8 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m473.8/473.8 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/156.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m156.8/156.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/93.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m93.7/93.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m55.6/55.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m46.2/46.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m208.3/208.3 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import init_chat_model"
      ],
      "metadata": {
        "id": "YTbbK4n85hI_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = init_chat_model(model=\"gemini-2.5-flash-lite\", model_provider=\"google-genai\")"
      ],
      "metadata": {
        "id": "G7toxIi_5qlO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\"tell me joke\""
      ],
      "metadata": {
        "id": "_F06qqHDyEE0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response=llm.invoke(prompt)"
      ],
      "metadata": {
        "id": "HqRfjBvyyHM0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqXPclayyJk-",
        "outputId": "ee5207ea-1ff1-4b23-a2d3-c3dbe5194c25"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why did the scarecrow win an award?\n",
            "\n",
            "Because he was outstanding in his field!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XFjlRJ9yhSO",
        "outputId": "df43d7db-d310-4530-c98f-998051d4e5e6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Why did the scarecrow win an award?\\n\\nBecause he was outstanding in his field!', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--6241483b-5d7a-4ce7-a96b-9e8eac0710c8-0', usage_metadata={'input_tokens': 4, 'output_tokens': 18, 'total_tokens': 22, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SDK"
      ],
      "metadata": {
        "id": "2Ui7-TExzl3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU google-genai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKyES2vzzfWH",
        "outputId": "8d0a3541-5bc3-40a5-fb6a-d2b4acfb3d6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m257.3/257.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "\n",
        "client = genai.Client()\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    contents=\"How does AI work?\"\n",
        ")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DeBK9228zdVN",
        "outputId": "82f6338b-6b77-4a3a-a26f-2238d35b1ec2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI, or Artificial Intelligence, works by enabling computer systems to perform tasks that typically require human intelligence. Instead of being explicitly programmed for every possible scenario, modern AI systems are designed to **learn from data, identify patterns, make decisions, and adapt** to new information.\n",
            "\n",
            "Here's a breakdown of how it generally works, moving from the core concept to the main approaches:\n",
            "\n",
            "---\n",
            "\n",
            "### The Core Idea: Learning from Data\n",
            "\n",
            "At its heart, most modern AI (especially Machine Learning and Deep Learning) revolves around the idea of **learning from examples.**\n",
            "\n",
            "Imagine teaching a child to recognize a cat. You don't give them a detailed list of rules (\"if it has pointy ears, whiskers, a tail, and meows, it's a cat\"). Instead, you show them many pictures of cats, point them out, and correct them when they make a mistake. Over time, the child learns to identify a cat on their own, even if it's a breed they haven't seen before.\n",
            "\n",
            "AI works similarly:\n",
            "\n",
            "1.  **Data is the Fuel:** AI systems are fed vast amounts of relevant data (e.g., images, text, numbers, sounds).\n",
            "2.  **Algorithms are the Recipes:** These are sets of instructions that the AI uses to process the data, find relationships, and build a \"model.\"\n",
            "3.  **Models are the Brains:** The result of an algorithm processing data is a trained \"model.\" This model has learned patterns and rules from the data, which it can then use to make predictions or decisions on *new, unseen data*.\n",
            "\n",
            "---\n",
            "\n",
            "### Key Components of an AI System\n",
            "\n",
            "*   **Data:** The raw information AI learns from. It needs to be relevant, clean, and often labeled (e.g., images tagged \"cat\" or \"dog\").\n",
            "*   **Algorithms:** The mathematical procedures and logical rules that process data and build the model.\n",
            "*   **Computational Power:** Modern AI, especially Deep Learning, requires significant processing power (CPUs and often GPUs) to crunch large datasets and train complex models.\n",
            "*   **The Model:** The output of the training process, embodying the learned knowledge from the data.\n",
            "\n",
            "---\n",
            "\n",
            "### Main Branches of AI (How they work in different ways)\n",
            "\n",
            "While \"AI\" is a broad term, most practical applications fall into these categories:\n",
            "\n",
            "1.  **Rule-Based AI (Symbolic AI / Expert Systems):**\n",
            "    *   **How it works:** Programmers explicitly define a set of \"if-then\" rules. For example, \"IF temperature > 100 AND patient has cough THEN diagnose 'flu'.\"\n",
            "    *   **Strengths:** Transparent, predictable, good for well-defined problems with clear rules.\n",
            "    *   **Limitations:** Doesn't learn, can't handle ambiguity, becomes unwieldy for complex problems with many exceptions.\n",
            "    *   **Examples:** Early chatbots, some diagnostic systems.\n",
            "\n",
            "2.  **Machine Learning (ML):**\n",
            "    *   **How it works:** Algorithms are trained on data to *learn* patterns and make predictions *without being explicitly programmed* for every outcome. It's about finding relationships in data.\n",
            "    *   **Sub-types of ML:**\n",
            "        *   **a. Supervised Learning:**\n",
            "            *   **How it works:** The AI is given **labeled data** (input-output pairs). It learns to map inputs to correct outputs.\n",
            "            *   **Example:** Showing it millions of pictures of \"cat\" and \"not-cat,\" along with the correct label.\n",
            "            *   **Goal:** Predict the label/value for new, unseen input.\n",
            "            *   **Common Tasks:** **Classification** (spam/not spam, disease/no disease), **Regression** (predicting house prices, stock values).\n",
            "        *   **b. Unsupervised Learning:**\n",
            "            *   **How it works:** The AI is given **unlabeled data** and tasked with finding hidden structures, patterns, or relationships within it on its own.\n",
            "            *   **Example:** Giving it millions of customer buying habits and asking it to group similar customers.\n",
            "            *   **Goal:** Discover patterns, group similar items.\n",
            "            *   **Common Tasks:** **Clustering** (customer segmentation), **Dimensionality Reduction** (simplifying complex data).\n",
            "        *   **c. Reinforcement Learning (RL):**\n",
            "            *   **How it works:** The AI (an \"agent\") learns by trial and error in an environment. It receives **rewards** for desirable actions and **penalties** for undesirable ones.\n",
            "            *   **Example:** Training an AI to play a video game, where points are rewards and losing is a penalty.\n",
            "            *   **Goal:** Maximize cumulative reward over time.\n",
            "            *   **Common Tasks:** Game playing (AlphaGo), robotics, self-driving cars (in simulations).\n",
            "\n",
            "3.  **Deep Learning (DL):**\n",
            "    *   **How it works:** A sub-field of Machine Learning that uses **Artificial Neural Networks (ANNs)** with many layers (hence \"deep\"). These networks are inspired by the structure and function of the human brain.\n",
            "    *   **Power:** Excellent at learning complex patterns directly from raw, unstructured data like images, audio, and text, often automatically identifying relevant \"features.\"\n",
            "    *   **Example:** Recognizing faces in photos, understanding spoken language, translating text.\n",
            "    *   **Common Tasks:** Image recognition, Natural Language Processing (NLP), speech recognition, drug discovery.\n",
            "\n",
            "---\n",
            "\n",
            "### The Training Process (for ML/DL)\n",
            "\n",
            "1.  **Data Collection & Preparation:** Gather and clean large datasets. For supervised learning, data must be accurately labeled. Data is split into training, validation, and test sets.\n",
            "2.  **Model Selection:** Choose an appropriate algorithm/neural network architecture (e.g., a Convolutional Neural Network for images, a Recurrent Neural Network for text).\n",
            "3.  **Training:**\n",
            "    *   The model is fed the **training data**.\n",
            "    *   It makes predictions, compares them to the correct answers (for supervised learning), and calculates the \"error\" (how wrong it was).\n",
            "    *   Using an **optimizer** and a **loss function**, the model adjusts its internal parameters (weights and biases) to *reduce* that error.\n",
            "    *   This process iterates thousands or millions of times until the model's error rate is minimized.\n",
            "4.  **Evaluation:** The trained model's performance is tested on the **validation** and **test data** (which it has never seen before) to ensure it can generalize well to new situations.\n",
            "5.  **Deployment:** Once the model performs satisfactorily, it can be integrated into applications to make real-time predictions or decisions.\n",
            "\n",
            "---\n",
            "\n",
            "### In Simple Terms:\n",
            "\n",
            "AI (especially modern AI) is essentially a highly sophisticated pattern-matching machine. You show it a massive amount of examples of what you want it to learn, and it figures out the underlying rules and patterns itself. Once it has learned these, it can then apply them to new, unseen information to make intelligent decisions or predictions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## HomeWORK:  Prompting models.\n",
        "\n",
        "## write a a code with GROQ models with Groq SDK and Langhchain frame ."
      ],
      "metadata": {
        "id": "nk41eL_g-G_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Demonstrate Transfomer Architecture uisng Encoder and Decoder"
      ],
      "metadata": {
        "id": "nS32JnNr0LqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Synthetic DATA: These are our \"dictionaries\" and \"rules\".\n",
        "\n",
        "# In a real model, all of this is learned from data.\n",
        "\n",
        "word_vectors = {\n",
        "\n",
        "    \"I\": [1.0, 2.0, 3.0], \"am\": [4.0, 5.0, 6.0],\n",
        "\n",
        "    \"a\": [7.0, 8.0, 9.0], \"student\": [10.0, 11.0, 12.0]\n",
        "\n",
        "}\n",
        "\n",
        "position_vectors = {\n",
        "\n",
        "    0: [0.1, 0.1, 0.9], 1: [0.1, 0.9, 0.1],\n",
        "\n",
        "    2: [0.9, 0.1, 0.1], 3: [0.9, 0.9, 0.1]\n",
        "\n",
        "}\n",
        "\n",
        "target_vocabulary = [\"at \", \"University of\", \"California\",\"GenAI Training Program\",\"dadfafsdf\",\"<end>\"]"
      ],
      "metadata": {
        "id": "-75bqkZQcQtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PART 1: THE ENCODER (Reads and understands the input)\n",
        "\n",
        "def encoder(sentence):\n",
        "\n",
        "    print(\"\\n--- Encoder's Turn: Understanding the Input ---\")\n",
        "\n",
        "    words = sentence.split()\n",
        "\n",
        "\n",
        "\n",
        "    # Step 1 & 2: Turn words into numbers and add position info.\n",
        "\n",
        "    print(\"1. Turning words into vectors and adding position info.\")\n",
        "\n",
        "    input_vectors = []\n",
        "\n",
        "    for i, word in enumerate(words):\n",
        "\n",
        "        vector = [w + p for w, p in zip(word_vectors[word], position_vectors[i])]\n",
        "\n",
        "        input_vectors.append(vector)\n",
        "\n",
        "        print(f\"  - '{word}' -> {vector}\")\n",
        "\n",
        "\n",
        "\n",
        "    # Step 3: Self-Attention (The magic part!)\n",
        "\n",
        "    print(\"\\n2. Finding word relationships with Self-Attention.\")\n",
        "\n",
        "    def self_attention(vectors):\n",
        "\n",
        "        output_vectors = []\n",
        "\n",
        "        for i in range(len(vectors)):\n",
        "\n",
        "            # This is a huge simplification of attention. We just average everything\n",
        "\n",
        "            # to show that each word is influenced by all others.\n",
        "\n",
        "            avg_vector = [sum(vals) / len(vals) for vals in zip(*vectors)]\n",
        "\n",
        "            output_vectors.append(avg_vector)\n",
        "\n",
        "            print(f\"  - Word {i+1} gets a new 'context-rich' vector: {avg_vector}\")\n",
        "\n",
        "        return output_vectors\n",
        "\n",
        "\n",
        "\n",
        "    encoder_notes = self_attention(input_vectors)\n",
        "\n",
        "    print(\"\\nEncoder is done! It has created its 'notes'.\")\n",
        "\n",
        "    return encoder_notes\n",
        "\n"
      ],
      "metadata": {
        "id": "tR1OlmCLcINi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PART 2: THE DECODER (Writes the output)\n",
        "\n",
        "def decoder(encoder_notes):\n",
        "\n",
        "    print(\"\\n--- Decoder's Turn: Writing the Output ---\")\n",
        "\n",
        "    output_sentence = []\n",
        "\n",
        "    current_word = \"<start>\"\n",
        "\n",
        "\n",
        "\n",
        "    while current_word != \"<end>\" and len(output_sentence) < 5:\n",
        "\n",
        "        print(f\"\\nDecoding step {len(output_sentence) + 1}:\")\n",
        "\n",
        "        print(f\"  - Words written so far: {output_sentence}\")\n",
        "\n",
        "        print(\"  - Decoder looks at its own previous words (Masked Self-Attention).\")\n",
        "\n",
        "        print(\"  - Decoder looks at the Encoder's notes for clues (Cross-Attention).\")\n",
        "\n",
        "\n",
        "\n",
        "        # In a real model, a complex calculation uses the encoder_notes to pick the best word.\n",
        "\n",
        "        # We will just pick the next word from our list to simulate this.\n",
        "\n",
        "        next_word_index = len(output_sentence)\n",
        "\n",
        "        next_word = target_vocabulary[next_word_index]\n",
        "\n",
        "        print(f\"  - Based on all that, it decides the next word is: '{next_word}'\")\n",
        "\n",
        "\n",
        "\n",
        "        output_sentence.append(next_word)\n",
        "\n",
        "        current_word = next_word\n",
        "\n",
        "\n",
        "\n",
        "    return \" \".join(output_sentence[:-1])"
      ],
      "metadata": {
        "id": "jEjto8QCcbWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# THE MAIN FUNCTION\n",
        "\n",
        "def generate(sentence):\n",
        "\n",
        "    print(f\"--- Starting Sentence: '{sentence}' ---\")\n",
        "\n",
        "    encoder_notes = encoder(sentence)\n",
        "\n",
        "    gen_sentence = decoder(encoder_notes)\n",
        "\n",
        "    print(f\"\\n--- complettion Sentence --- \\n {sentence} {gen_sentence}\")\n",
        "\n",
        "    return gen_sentence\n",
        "\n",
        "\n",
        "\n",
        "# RUN THE STORY\n",
        "\n",
        "generate(\"I am a student\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921
        },
        "id": "_56rqx5icgYL",
        "outputId": "2506d2eb-c661-43c2-c99b-5583ac5b6032"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Sentence: 'I am a student' ---\n",
            "\n",
            "--- Encoder's Turn: Understanding the Input ---\n",
            "1. Turning words into vectors and adding position info.\n",
            "  - 'I' -> [1.1, 2.1, 3.9]\n",
            "  - 'am' -> [4.1, 5.9, 6.1]\n",
            "  - 'a' -> [7.9, 8.1, 9.1]\n",
            "  - 'student' -> [10.9, 11.9, 12.1]\n",
            "\n",
            "2. Finding word relationships with Self-Attention.\n",
            "  - Word 1 gets a new 'context-rich' vector: [6.0, 7.0, 7.8]\n",
            "  - Word 2 gets a new 'context-rich' vector: [6.0, 7.0, 7.8]\n",
            "  - Word 3 gets a new 'context-rich' vector: [6.0, 7.0, 7.8]\n",
            "  - Word 4 gets a new 'context-rich' vector: [6.0, 7.0, 7.8]\n",
            "\n",
            "Encoder is done! It has created its 'notes'.\n",
            "\n",
            "--- Decoder's Turn: Writing the Output ---\n",
            "\n",
            "Decoding step 1:\n",
            "  - Words written so far: []\n",
            "  - Decoder looks at its own previous words (Masked Self-Attention).\n",
            "  - Decoder looks at the Encoder's notes for clues (Cross-Attention).\n",
            "  - Based on all that, it decides the next word is: 'at '\n",
            "\n",
            "Decoding step 2:\n",
            "  - Words written so far: ['at ']\n",
            "  - Decoder looks at its own previous words (Masked Self-Attention).\n",
            "  - Decoder looks at the Encoder's notes for clues (Cross-Attention).\n",
            "  - Based on all that, it decides the next word is: 'Livermore'\n",
            "\n",
            "Decoding step 3:\n",
            "  - Words written so far: ['at ', 'Livermore']\n",
            "  - Decoder looks at its own previous words (Masked Self-Attention).\n",
            "  - Decoder looks at the Encoder's notes for clues (Cross-Attention).\n",
            "  - Based on all that, it decides the next word is: 'Temple'\n",
            "\n",
            "Decoding step 4:\n",
            "  - Words written so far: ['at ', 'Livermore', 'Temple']\n",
            "  - Decoder looks at its own previous words (Masked Self-Attention).\n",
            "  - Decoder looks at the Encoder's notes for clues (Cross-Attention).\n",
            "  - Based on all that, it decides the next word is: 'GenAI Training Program'\n",
            "\n",
            "Decoding step 5:\n",
            "  - Words written so far: ['at ', 'Livermore', 'Temple', 'GenAI Training Program']\n",
            "  - Decoder looks at its own previous words (Masked Self-Attention).\n",
            "  - Decoder looks at the Encoder's notes for clues (Cross-Attention).\n",
            "  - Based on all that, it decides the next word is: 'dadfafsdf'\n",
            "\n",
            "--- complettion Sentence --- \n",
            " I am a student at  Livermore Temple GenAI Training Program\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'at  Livermore Temple GenAI Training Program'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Home work\n",
        "\n",
        "# 1. Try with Groq API key with meta models\n",
        "# 2. Try with OPENAI_API_KEY with gpt-4o-minin models\n",
        "#    add variables into prompt\n",
        "# prompt='tell me a joke about roman empire'. ---> (make topic a variable)\n",
        "#  Use Langchain framework"
      ],
      "metadata": {
        "id": "4U3cvT6q7F_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "\n",
        "client = genai.Client()\n"
      ],
      "metadata": {
        "id": "Kyqm_jYs8YN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt='tell me a joke about {topic}'.format(topic='dog')"
      ],
      "metadata": {
        "id": "YPHE4J6k-SAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    contents=prompt,\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3zibd1c-O2X",
        "outputId": "8911e08b-61ee-4758-d76c-53508f910932"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why did the dog sit in the shade?\n",
            "\n",
            "Because he didn't want to be a hot dog!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq\n",
        "\n",
        "client = Groq()"
      ],
      "metadata": {
        "id": "veEAUflv-aPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "completion = client.chat.completions.create(\n",
        "    model=\"gemma2-9b-it\",\n",
        "    messages=[\n",
        "      {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "        }\n",
        "    ],\n",
        ")"
      ],
      "metadata": {
        "id": "wfdasrSq-j6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "completion"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNd6pqew-3aH",
        "outputId": "52095e4b-2e6f-48e8-d81a-8372de15df74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-9f977d6e-2cbf-4f10-acb9-765def94d615', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Why do dogs run in circles? \\n\\nBecause it's really hard to run in squares! üê∂ üòÑ \\n\", role='assistant', executed_tools=None, function_call=None, reasoning=None, tool_calls=None))], created=1757077531, model='gemma2-9b-it', object='chat.completion', system_fingerprint='fp_10c08bf97d', usage=CompletionUsage(completion_tokens=27, prompt_tokens=15, total_tokens=42, completion_time=0.049090909, prompt_time=0.001248509, queue_time=0.092400722, total_time=0.050339418), usage_breakdown=None, x_groq={'id': 'req_01k4czptaef1ervbj0r61zt6hp'}, service_tier='on_demand')"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JbGfEBf-yQ3",
        "outputId": "c2a84225-af58-4d46-f663-dcc0aedcd851"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why do dogs run in circles? \n",
            "\n",
            "Because it's really hard to run in squares! üê∂ üòÇ \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## OPENAI"
      ],
      "metadata": {
        "id": "z6Gj6hW3_yby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ['OPENAI_API_KEY']=userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "Jd_ktreSAIHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU openai"
      ],
      "metadata": {
        "id": "DUp9iFxt_0dJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e2f6737-31cc-49ee-f620-f72e667042de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.2/1.0 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.7/1.0 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI()"
      ],
      "metadata": {
        "id": "DdSYqmDT_9HF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt='Write a one-sentence bedtime story about a unicorn.'"
      ],
      "metadata": {
        "id": "yPUYUhjN10-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.responses.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    input=prompt\n",
        ")\n",
        "print(response.output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jY1xgjBm__LB",
        "outputId": "f4fba0dd-701d-4393-e3f2-08236a0a2825"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As the silver moonlight bathed the enchanted forest, a gentle unicorn spread her shimmering wings and soared across the starry sky, granting sweet dreams to all who believed in magic.\n"
          ]
        }
      ]
    }
  ]
}